\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref, url}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry} 

\title{A Learning Augmented approach to Cardinality Estimation}
\author{Mărcuș Alexandru Marian}

\begin{document}
\maketitle
\section{Schita cuprins}
Abstract\\
Introduction\\
Related Work\\
Theoretical Foundation\\
Methodology\\
Implementation\\
Experiments\\
Conclusions\\


\section{Theoretical Foundation}




consistency, robustness, competitiveness

loss function: log loss/ cross entropy
$ L = (\log(E)-\log(N))^2 $

\section{Plan pentru partea aplicativa a lucrarii}
Implementarea standard a algoritmelor HLL si HLL++.\\
Generare de date cu diverse distributii ce ar putea fi regasite in date reale (ex. uniform, clustered etc).\\
Feature extraction din HLL sketch (ex. max, min, media, devia standard, \% din registre sunt goale, histograma a valorilor din registrii).\\
Antrenarea unui model mic (ex. regresie, un neural network cu putine layere etc).\\
Evaluarea modelului fata de standarde ca si acuratete, runtime si memorie.\\
Analiza rezultatelor si concluzii.\\


\section{Methodology}

\subsection{Experimental Setup}
To measure the performance of this method, we generated 200000 different HyperLogLog sketches, each sketch having
a cardinality in the range of $[10, 10^8]$, with a log-uniform distribution. Assuming a uniform hash function, we can 
generate uniformly distributed 64 bit integers, simulating the hash values of random elements. 
For real-world datasets we are using

\subsection{Model matematic}
\begin{algorithm}[H]
\caption{Learned HyperLogLog}
\begin{algorithmic}[1]
\Require Let $h : \mathcal{D} \to \{0,1\}^{64}$ hash data from domain $\mathcal{D}$.  
Let $m = 2^p$ with $p \in [4..16]$.

\Statex
\textbf{Phase 0: Initialization.}
\State Initialize $m$ registers $M[0]$ to $M[m-1]$ to 0.

\Statex
\textbf{Phase 1: Aggregation.}
\ForAll{$v \in S$}
    \State $x := h(v)$
    \State $\mathit{id} := (x_{63},\ldots,x_{64-p})_2$ \Comment{First $p$ bits of $x$}
    \State $w := (x_{63-p},\ldots,x_0)_2$
    \State $M[\mathit{id}] := \max(M[\mathit{id}], \rho(w))$
\EndFor

\Statex
\textbf{Phase 2: Result computation.}
\State \Return Model Prediction 
\end{algorithmic}
\end{algorithm}

Instead of using the classic formula $E := \alpha_m m^2 \left( \sum_{j=0}^{m-1} 2^{-M[j]} \right)^{-1}$ \cite{flajolet2007hyperloglog}
for extracting the result from our sketch, we propose training a neural network to predict the cardinality. This should have several
advantages, such as reduced bias and better results for real world distributions. As a bonus, we also eliminate the need 
for magic numbers and special cases of the original algorithm. This approach should not have much overhead, in terms of memory or speed.

For the features of the model we are using
As the loss function, we chose the Mean Squared Logarithmic Error $L = \frac{1}{n} \sum_{i=0}^n \left( \log(1+\hat{N}) - \log(1+N) \right)$
\subsection{Results Validation}
For validating our results we are plotting the 
Relative Error $error := \frac{\vert \hat{N} - N \vert}{N}$.
In the literature, the "raw" HLL estimate shows different performances
for different ranges of cardinality, that are often empirically determined
and corrected. We compare our results to the classic HLL \cite{flajolet2007hyperloglog},
HyperLogLog++ \cite{heule2013hyperloglog} and other learned methods 
\cite{aiyou2011selflearningbitmap} \cite{wu2021learnedestimator}.
The ideal result would be an error close to the theoretical limit $1.04/sqrt(m)$



\nocite{*}

\bibliographystyle{unsrt}
\bibliography{references}
\end{document}