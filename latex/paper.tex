\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref, url}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry} 

\title{A Learning Augmented approach to Cardinality Estimation}
\author{Mărcuș Alexandru Marian}

\begin{document}
\maketitle

\section{Methodology}

\subsection{Experimental Setup}
To measure the performance of this method, we generated a synthetic dataset
containing 200000 different HyperLogLog sketches, each sketch having
a cardinality in the range of $[10, 10^8]$, with a log-uniform distribution. Assuming a uniform hash function, we can 
generate uniformly distributed 64 bit integers, simulating the hash values of random elements. 
For real-world datasets we are using Openadresses and NCVoter. \cite{harmouch2017cesurvey}

\subsection{Model matematic}
\begin{algorithm}[H]
\caption{Learned HyperLogLog}
\begin{algorithmic}[1]
\Require Let $h : \mathcal{D} \to \{0,1\}^{64}$ hash data from domain $\mathcal{D}$.  
Let $m = 2^p$ with $p \in [4..16]$.

\Statex
\textbf{Phase 0: Initialization.}
\State Initialize $m$ registers $M[0]$ to $M[m-1]$ to 0.

\Statex
\textbf{Phase 1: Aggregation.}
\ForAll{$v \in S$}
    \State $x := h(v)$
    \State $\mathit{id} := (x_{63},\ldots,x_{64-p})_2$ \Comment{First $p$ bits of $x$}
    \State $w := (x_{63-p},\ldots,x_0)_2$
    \State $M[\mathit{id}] := \max(M[\mathit{id}], \rho(w))$
\EndFor

\Statex
\textbf{Phase 2: Result computation.}
\State \Return Model Prediction 
\end{algorithmic}
\end{algorithm}

Instead of using the classic formula $E := \alpha_m m^2 \left( \sum_{j=0}^{m-1} 2^{-M[j]} \right)^{-1}$ \cite{flajolet2007hyperloglog}
for extracting the result from our sketch, we propose training a neural network to predict the cardinality. 
This approach should not have much overhead, in terms of memory or speed.

The model takes the raw register array $M \in \left\{ 0, 1, ..., q \right\}^m$
as input and outputs the prediction $\log(N)$.

Each HLL sketch consists of $m=2^p$ registers, 
$M = \left[ M_1, M_2, ..., M_m \right]$, $M_i\in \left\{ 0, 1, ..., q \right\}$

Before feeding the array $M$ to the network, we normalize the values $x_i=M_i/q$.
We apply three 1D convolutional layers, to downsample from the $2^p$ channels
down to a compact 64x1024 activation map.
Then it is fed into a multilayer perceptron, with three linear layers, from 
64*1024 -> 512 -> 128 -> final prediction, $\log(n)$. 

As the loss function, we chose the mean squared logarithmic error 
$$L = \frac{1}{n} \sum_{i=0}^n \left( \log(1+\hat{N}) - \log(1+N) \right)$$
\subsection{Results Validation}
For validating our results we are plotting the 
relative error $error := \frac{\vert \hat{N} - N \vert}{N}$.
In the literature, the "raw" HLL estimate shows different performances
for different ranges of cardinality, that are often empirically determined
and corrected. It's important we observe the performance of our approach for
all the possible different cardinalities, thus we prefer plotting our results
instead of another measure like the mean relative error.

\section{Studiu de caz}

On our synthetic dataset we obtain better results than the original HLL paper,
as shown in our comparison graph\ref{fig:comparison}.
We still have to measure the overhead produces by this approach, even 
if the theoretical space and time complexity remains the same.

\begin{figure}
\centering
\includegraphics[totalheight=8cm]{result.png}
\caption{Our Approach (orange) vs Original HLL (blue)}
\label{fig:comparison}
\end{figure}

\section{Related work}
We compare our results to the classic HLL \cite{flajolet2007hyperloglog},
HyperLogLog++ \cite{heule2013hyperloglog} and other learned methods 
\cite{aiyou2011selflearningbitmap} \cite{wu2021learnedestimator}.
The ideal result would be an error close to the theoretical limit $1.04/sqrt(m)$.

Our approach builds the HyperLogLog sketch like usual, as proposed by
\cite{flajolet2007hyperloglog}, but extracts the cardinality information
using a machine learning approach. This is in contrast to other machine learning
approaches that try to sample from the dataset to estimate its cardinality.
Building the sketch requires a scan of the dataset so it is better suited
for streaming environments. A more comprehensive review is found 
in \cite{harmouch2017cesurvey}.

The advantages of this approach are reduced bias for real world distribution, because the hash function
may not be able to produce fully uniform results, but the network could learn
this and not be affected. It also eliminates the need 
for magic numbers and special cases, used in the original algorithm and other
improvements \cite{heule2013hyperloglog}.


\nocite{*}

\bibliographystyle{unsrt}
\bibliography{references}
\end{document}